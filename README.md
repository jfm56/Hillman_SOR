# Hillmann AI System

A secure, **fully local** web application for generating Hillmann Site Observation Reports (SOR) using AI-powered image analysis, document parsing, and style-learning draft generation.

**üîí Privacy First**: All AI processing runs locally using Ollama - no data ever leaves your network.

## Features

| Feature | Description |
|---------|-------------|
| **Local LLM (Ollama)** | 100% on-premises AI - no external API calls, PHI-safe |
| **Photo Upload & Analysis** | AI analyzes site photos for conditions, materials, and safety issues |
| **PDF Document Parsing** | Extract structured data from plans, cost reviews, change orders, and prior SORs |
| **Style Learning** | Upload past reports to teach the AI your writing style |
| **RAG Draft Generation** | Generate drafts matching Hillmann's writing style from learned reports |
| **AI Chat Assistant** | Private chat for rewriting and improving report sections |
| **Role-Based Access** | ADMIN, MANAGER, INSPECTOR roles with appropriate permissions |
| **Full Audit Logging** | Track all user actions and AI interactions for compliance |

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      SOR AI SYSTEM (100% Local)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ   Frontend   ‚îÇ    ‚îÇ   Backend    ‚îÇ    ‚îÇ   Database   ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ   Next.js    ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   FastAPI    ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ  PostgreSQL  ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ   Port 3000  ‚îÇ    ‚îÇ   Port 8000  ‚îÇ    ‚îÇ  + pgvector  ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                             ‚îÇ                                        ‚îÇ
‚îÇ                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ                      ‚îÇ AI Services ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Ollama     ‚îÇ            ‚îÇ
‚îÇ                      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ  Local LLM   ‚îÇ            ‚îÇ
‚îÇ                      ‚îÇ ‚Ä¢ Chat      ‚îÇ    ‚îÇ  Port 11434  ‚îÇ            ‚îÇ
‚îÇ                      ‚îÇ ‚Ä¢ RAG       ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                      ‚îÇ ‚Ä¢ Draft Gen ‚îÇ           ‚îÇ                     ‚îÇ
‚îÇ                      ‚îÇ ‚Ä¢ Style     ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ                      ‚îÇ   Learning  ‚îÇ    ‚îÇ   Models    ‚îÇ             ‚îÇ
‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ ‚Ä¢ llama3.2  ‚îÇ             ‚îÇ
‚îÇ                             ‚îÇ           ‚îÇ ‚Ä¢ nomic-    ‚îÇ             ‚îÇ
‚îÇ                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ   embed     ‚îÇ             ‚îÇ
‚îÇ                      ‚îÇFile Storage ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ                      ‚îÇ ./storage/  ‚îÇ                                 ‚îÇ
‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Quick Start

### Prerequisites

- **Docker Desktop** (includes Docker Compose)
- **~6GB disk space** for AI models

### 1. Start All Services

```bash
cd sor-ai-system
docker-compose up -d --build
```

### 2. Download AI Models (One-Time Setup)

```bash
# Download chat model (~2GB)
docker exec sor_ollama ollama pull llama3.2

# Download embedding model (~274MB)
docker exec sor_ollama ollama pull nomic-embed-text
```

### 3. Verify All Services Are Running

```bash
docker-compose ps
```

Expected output:
```
NAME           SERVICE    STATUS           PORTS
sor_backend    backend    Up               0.0.0.0:8000->8000/tcp
sor_frontend   frontend   Up               0.0.0.0:3000->3000/tcp
sor_ollama     ollama     Up               0.0.0.0:11434->11434/tcp
sor_postgres   db         Up (healthy)     0.0.0.0:5432->5432/tcp
```

### 5. Access the Application

| Service | URL | Description |
|---------|-----|-------------|
| **Frontend** | http://localhost:3000 | Main application UI |
| **API Docs** | http://localhost:8000/docs | Interactive Swagger documentation |
| **Health Check** | http://localhost:8000/health | API health status |

## Troubleshooting

### Backend Not Starting

Check logs:
```bash
docker logs sor_backend
```

Common issues:
- **OPENAI_API_KEY not set**: Create `backend/.env` with your API key
- **Database connection failed**: Ensure PostgreSQL is healthy first

### Restart Services

```bash
docker-compose restart backend
docker-compose restart frontend
```

### Full Rebuild

```bash
docker-compose down
docker-compose up -d --build
```

### View All Logs

```bash
docker-compose logs -f
```

## Development Setup

### Backend (FastAPI)

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment file
cp .env.example .env
# Edit .env with your settings

# Start PostgreSQL (using Docker)
docker run -d --name sor_postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=sor_ai \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run the server
uvicorn app.main:app --reload
```

### Frontend (Next.js)

```bash
cd frontend

# Install dependencies
npm install

# Copy environment file
cp .env.example .env.local
# Edit .env.local with your settings

# Run development server
npm run dev
```

## API Endpoints

### Authentication
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/auth/login` | Login |
| GET | `/api/v1/auth/me` | Get current user |

### Projects
| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/v1/projects` | List projects |
| POST | `/api/v1/projects` | Create project |
| GET | `/api/v1/projects/{id}` | Get project |

### File Upload
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/upload/images` | Upload photos |
| POST | `/api/v1/upload/documents` | Upload PDFs |

### AI Pipeline
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/ai/analyze-image` | Analyze image |
| POST | `/api/v1/ai/parse-document` | Parse PDF |
| POST | `/api/v1/ai/generate-draft` | Generate section |
| POST | `/api/v1/ai/rewrite` | Rewrite text |

### Reports
| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/v1/reports` | List reports |
| POST | `/api/v1/reports` | Create report |
| POST | `/api/v1/reports/{id}/generate` | Generate AI drafts |

See full API documentation at http://localhost:8000/docs

## Project Structure

```
sor-ai-system/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/v1/endpoints/    # API routes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/                # Config, security
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db/                  # Database session
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/              # SQLAlchemy models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/            # Business logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ai/              # AI services
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py              # FastAPI app
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app/                 # Next.js pages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/          # React components
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lib/                 # Utilities
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ   ‚îú‚îÄ‚îÄ DATABASE_SCHEMA.md
‚îÇ   ‚îú‚îÄ‚îÄ API_ENDPOINTS.md
‚îÇ   ‚îî‚îÄ‚îÄ AI_PIPELINE.md
‚îú‚îÄ‚îÄ storage/                     # File storage
‚îî‚îÄ‚îÄ docker-compose.yml
```

## AI Pipeline

1. **Ingestion**: Upload photos/PDFs, assign to project/building
2. **Image Analysis**: Vision model extracts conditions, materials, safety issues
3. **Document Parsing**: Extract structured data from PDFs
4. **Style Learning (RAG)**: Retrieve similar historical sections
5. **Draft Generation**: LLM synthesizes all inputs into report sections

## Security

- JWT authentication
- Role-based access control (admin, manager, inspector, reviewer)
- All AI processing on-premises (no external data exposure)
- Full audit logging for compliance
- Database encryption at rest

## Environment Variables

### Backend
| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | PostgreSQL connection string | Auto-configured |
| `SECRET_KEY` | JWT signing key | Auto-generated |
| `USE_LOCAL_LLM` | Use local Ollama instead of OpenAI | `true` |
| `OLLAMA_HOST` | Ollama server URL | `http://ollama:11434` |
| `LOCAL_MODEL` | Chat model name | `llama3.2` |
| `LOCAL_EMBEDDING_MODEL` | Embedding model name | `nomic-embed-text` |
| `STORAGE_PATH` | File storage path | `./storage` |
| `CORS_ORIGINS` | Allowed origins | `http://localhost:3000` |

### Frontend
| Variable | Description | Default |
|----------|-------------|---------|
| `NEXT_PUBLIC_API_URL` | Backend API URL | `http://localhost:8000/api/v1` |

## Default Login

| Field | Value |
|-------|-------|
| Email | `admin@hillmann.com` |
| Password | `admin123` |

**Change the default password after first login.**

## User Roles

| Role | Permissions |
|------|-------------|
| **ADMIN** | Full system access, user management |
| **MANAGER** | Review and approve reports |
| **INSPECTOR** | Upload photos, write narratives |

## Server Deployment

### Requirements

| Resource | Minimum | Recommended |
|----------|---------|-------------|
| CPU      | 4 cores | 8 cores     |
| RAM      | 8 GB    | 16 GB       |
| Storage  | 20 GB   | 50 GB       |
| OS       | Ubuntu 20.04+ | Ubuntu 22.04 |

### Quick Deploy

```bash
# SSH into your server
ssh user@YOUR_SERVER_IP

# Clone the repository
git clone https://github.com/jfm56/Hillman_SOR.git hillmann-ai
cd hillmann-ai

# Run the deployment script
./deploy.sh
```

The script will:
1. Install Docker if needed
2. Generate secure passwords and secrets
3. Prompt you to set your server IP
4. Build and start all services
5. Pull the AI models (~2GB)

### Access After Deployment

- **URL**: `http://YOUR_SERVER_IP`
- **Login**: `admin@hillmann.com` / `admin123`

‚ö†Ô∏è **Change the admin password after first login!**

### Enable Auto-Deploy (CI/CD)

Set up automatic deployment when you push to GitHub:

```bash
# On the server, run:
./webhook/setup.sh
```

This outputs a **webhook secret**. Then configure GitHub:

1. Go to: https://github.com/jfm56/Hillman_SOR/settings/hooks
2. Click **Add webhook**
3. **Payload URL**: `http://YOUR_SERVER_IP:9000/webhook`
4. **Content type**: `application/json`
5. **Secret**: paste the secret from the setup script
6. **Events**: Just the push event
7. Click **Add webhook**

Now every `git push` automatically deploys to the server.

### Manual Update (Without Auto-Deploy)

```bash
ssh user@YOUR_SERVER_IP
cd hillmann-ai
git pull
docker compose -f docker-compose.prod.yml up -d --build
```

### Useful Commands

```bash
# View logs
docker compose -f docker-compose.prod.yml logs -f

# View specific service
docker compose -f docker-compose.prod.yml logs -f backend

# Restart services
docker compose -f docker-compose.prod.yml restart

# Stop all services
docker compose -f docker-compose.prod.yml down

# Full reset (deletes data)
docker compose -f docker-compose.prod.yml down -v
```

### Firewall Setup

```bash
sudo ufw allow 80/tcp    # HTTP
sudo ufw allow 443/tcp   # HTTPS (if using SSL)
sudo ufw allow 9000/tcp  # Webhook (if using auto-deploy)
sudo ufw enable
```

## License

Proprietary - Hillmann Consulting
